# lyrics_analyzer.py

#!/usr/bin/env python
# -*- coding: utf-8 -*-

import torch
from transformers import pipeline, AutoModelForSeq2SeqLM, AutoTokenizer
import argparse
import json
import os
import re
from collections import Counter
from typing import List, Tuple
import nltk
import deepl

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# --- ê²½ê³  ë©”ì‹œì§€ ìˆ¨ê¸°ê¸° ---
# Hugging Face í† í¬ë‚˜ì´ì € ë³‘ë ¬ ì²˜ë¦¬ ê²½ê³  ë¹„í™œì„±í™”
os.environ["TOKENIZERS_PARALLELISM"] = "false"

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# í™˜ê²½ ë³€ìˆ˜ / í† í° ì„¤ì •
from dotenv import load_dotenv  # --- ì¶”ê°€

# ë¡œì»¬ ê°œë°œ í™˜ê²½: .env íŒŒì¼ì—ì„œ í™˜ê²½ ë³€ìˆ˜ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.
load_dotenv()  # Cloud Runì—ëŠ” .env íŒŒì¼ì´ ì—†ìœ¼ë¯€ë¡œ ì´ ë¼ì¸ì€ ë¬´ì‹œë©ë‹ˆë‹¤.

# Cloud Run í˜¸í™˜: dotenv ëŒ€ì‹  í™˜ê²½ë³€ìˆ˜ ì§ì ‘ ì‚¬ìš©í•˜ëŠ” ì½”ë“œë¡œ ë³€ê²½
DEEPL_KEY = os.environ.get("DEEPL_KEY")
BART_PATH = "./models/bart"
T5_PATH = "./models/eenzeenee_t5"
# KOBART_PATH = "./models/kobart"

nltk.download("stopwords", quiet=True)
nltk.download("punkt", quiet=True)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# --- [ì¶”ê°€] ëª¨ë¸ ë° NLP ë„êµ¬ ì „ì—­ ë¡œë”© (íŒŒì¼ ìµœìƒë‹¨) ---
from sklearn.feature_extraction.text import CountVectorizer
from konlpy.tag import Okt

print("ğŸ”„ [Global Init] Loading NLP models and tools...")
try:
    # 1. ìš”ì•½ ëª¨ë¸ (BART)
    print("  Loading BART Model...")
    tokenizer_bart = AutoTokenizer.from_pretrained(BART_PATH)
    model_bart = AutoModelForSeq2SeqLM.from_pretrained(BART_PATH).to("cpu")
    summarizer_bart_pipeline = pipeline(
        "summarization", model=model_bart, tokenizer=tokenizer_bart
    )
    print("âœ… BART Model loaded.")

    # 2. ìš”ì•½ ëª¨ë¸ (T5)
    print("  Loading T5 Model...")
    tokenizer_t5 = AutoTokenizer.from_pretrained(T5_PATH)
    model_t5 = AutoModelForSeq2SeqLM.from_pretrained(T5_PATH).to("cpu")
    # T5ëŠ” pipeline ëŒ€ì‹  ì§ì ‘ generate ì‚¬ìš© (ì›ë˜ ì½”ë“œ ë°©ì‹ ìœ ì§€)
    print("âœ… T5 Model loaded.")

    # 3. DeepL ë²ˆì—­ê¸°
    translator_deepl = deepl.Translator(DEEPL_KEY) if DEEPL_KEY else None
    if translator_deepl:
        print("âœ… DeepL Translator loaded.")
    else:
        print("âš ï¸ Warning: DEEPL_KEY not set. Translation will be skipped.")

    # 4. ì˜ì–´ í‚¤ì›Œë“œ ë„êµ¬
    vectorizer_en = CountVectorizer(
        stop_words="english",
        token_pattern=r"(?u)\b[a-zA-Z]{3,}\b",  # 3ì ì´ìƒ ì•ŒíŒŒë²³
    )
    print("âœ… English Keyword Vectorizer loaded.")

    # 5. í•œêµ­ì–´ í‚¤ì›Œë“œ ë„êµ¬
    okt = Okt()
    print("âœ… Korean (Okt) Tokenizer loaded.")
    print("ğŸ‘ [Global Init] All models and tools loaded successfully.")

except Exception as e:
    print(f"âŒ [Global Init] Failed to load models: {e}")
    # ì‹¤íŒ¨ ì‹œ Noneìœ¼ë¡œ ì„¤ì • (api_server.pyì—ì„œ í™•ì¸ ê°€ëŠ¥)
    (
        summarizer_bart_pipeline,
        tokenizer_t5,
        model_t5,
        translator_deepl,
        vectorizer_en,
        okt,
    ) = (None,) * 6
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ---------------------------  1) ì–¸ì–´ ê°ì§€ --------------------------- #


def detect_language(text: str, hangul_weight: float = 0.5) -> str:
    """ê°€ì‚¬ì—ì„œ í•œê¸€Â·ì˜ë¬¸ ë¬¸ì ë¹„ìœ¨ë¡œ â€˜koâ€™/â€˜enâ€™ ë°˜í™˜."""
    # 1. í•œê¸€ ë° ì˜ë¬¸ì ì¶”ì¶œ
    hangul = re.findall(r"[ê°€-í£]", text)
    latin = re.findall(r"[A-Za-z]", text)

    # 2. ì˜ˆì™¸ ì²˜ë¦¬: í•œê¸€/ì˜ë¬¸ìê°€ ëª¨ë‘ ì—†ëŠ” ê²½ìš°
    if len(hangul) + len(latin) == 0:
        return "en"  # ê¸°ë³¸ê°’

    # 3. ì–¸ì–´ íŒë³„ ë¡œì§
    return "ko" if len(hangul) / (len(hangul) + len(latin)) >= hangul_weight else "en"


# ---------------------------  2) ìš”ì•½ --------------------------- #
# ì˜ì–´ â†’ BART summarizer


def summarize_en(text: str, max_len: int = 90, min_len: int = 25) -> str:
    # tokenizer = AutoTokenizer.from_pretrained(
    #     BART_PATH
    # )  # Cloud Run ì˜¤í”„ë¼ì¸ ìƒíƒœì´ë¯€ë¡œ ë‹¤ìš´, ìƒëŒ€ ê²½ë¡œë¡œ ì§€ì •
    # model = AutoModelForSeq2SeqLM.from_pretrained(BART_PATH).to("cpu")
    # with torch.no_grad():
    #     summarizer = pipeline("summarization", model=model, tokenizer=tokenizer)
    #     summary = summarizer(
    #         text, min_length=min_len, max_length=max_len, do_sample=False
    #     )[0]["summary_text"]
    # return summary.strip()

    # [ë³€ê²½] ì „ì—­ 'summarizer_bart_pipeline' ì‚¬ìš©
    if not summarizer_bart_pipeline:
        return "BART ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨"
    with torch.no_grad():
        summary = summarizer_bart_pipeline(
            text, min_length=min_len, max_length=max_len, do_sample=False
        )[0]["summary_text"]
    return summary.strip()


# í•œêµ­ì–´ â†’  t5-base-korean-summarization


def summarize_ko(text: str, max_len: int = 64, min_len: int = 10) -> str:
    # tokenizer = AutoTokenizer.from_pretrained(T5_PATH)
    # model = AutoModelForSeq2SeqLM.from_pretrained(T5_PATH).to("cpu")

    # prefix = "summarize: "
    # input_text = prefix + text.replace("\n", " ").strip()
    # inputs = tokenizer(
    #     [input_text], max_length=512, truncation=True, return_tensors="pt"
    # )

    # with torch.no_grad():
    #     output = model.generate(
    #         **inputs,
    #         num_beams=3,
    #         do_sample=True,
    #         min_length=min_len,
    #         max_length=max_len,
    #         early_stopping=True,
    #     )

    # decoded = tokenizer.batch_decode(output, skip_special_tokens=True)[0].strip()
    # sentences = nltk.sent_tokenize(decoded)
    # return " ".join(sentences[:3])

    # [ë³€ê²½] ì „ì—­ 'tokenizer_t5'ì™€ 'model_t5' ì‚¬ìš©
    if not tokenizer_t5 or not model_t5:
        return "T5 ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨"

    prefix = "summarize: "
    input_text = prefix + text.replace("\n", " ").strip()
    inputs = tokenizer_t5(
        [input_text], max_length=512, truncation=True, return_tensors="pt"
    )

    with torch.no_grad():
        output = model_t5.generate(
            **inputs,
            num_beams=3,
            do_sample=True,
            min_length=min_len,
            max_length=max_len,
            early_stopping=True,
        )
    decoded = tokenizer_t5.batch_decode(output, skip_special_tokens=True)[0].strip()
    sentences = nltk.sent_tokenize(decoded)
    return " ".join(sentences[:3])


# ---------------------------  3) DeepL ë²ˆì—­ --------------------------- #


def translate_to_ko(text: str) -> str:
    # translator = deepl.Translator(DEEPL_KEY)
    # return translator.translate_text(text, target_lang="KO").text
    # [ë³€ê²½] ì „ì—­ 'translator_deepl' ì‚¬ìš©
    if not translator_deepl:
        print("ë²ˆì—­ê¸° ì—†ìŒ. ì˜ì–´ ìš”ì•½ ì›ë³¸ ë°˜í™˜.")
        return text  # DeepL í‚¤ê°€ ì—†ìœ¼ë©´ ì˜ì–´ ì›ë³¸ ë°˜í™˜
    return translator_deepl.translate_text(text, target_lang="KO").text


# ---------------------------  4) ì£¼ìš” ë‹¨ì–´ ì¶”ì¶œ --------------------------- #


def keywords_en(text: str, top_k: int = 10) -> List[str]:
    # from sklearn.feature_extraction.text import CountVectorizer

    # vectorizer = CountVectorizer(
    #     stop_words="english",
    #     token_pattern=r"(?u)\b[a-zA-Z]{3,}\b",  # 3ì ì´ìƒ ì•ŒíŒŒë²³
    # )
    # X = vectorizer.fit_transform([text.lower()])
    # counts = X.toarray().sum(axis=0)
    # vocab = vectorizer.get_feature_names_out()
    # freq = sorted(zip(vocab, counts), key=lambda x: x[1], reverse=True)
    # return [w for w, _ in freq[:top_k]]

    # [ë³€ê²½] ì „ì—­ 'vectorizer_en' ì‚¬ìš©
    if not vectorizer_en:
        return ["ì˜ì–´ í‚¤ì›Œë“œ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨"]
    X = vectorizer_en.fit_transform([text.lower()])
    counts = X.toarray().sum(axis=0)
    vocab = vectorizer_en.get_feature_names_out()
    freq = sorted(zip(vocab, counts), key=lambda x: x[1], reverse=True)
    return [w for w, _ in freq[:top_k]]


def keywords_ko(text: str, top_k: int = 10) -> List[str]:
    # from konlpy.tag import Okt

    # okt = Okt()
    # nouns = [n for n in okt.nouns(text) if len(n) > 1]  # 2ê¸€ì ì´ìƒ
    # cnt = Counter(nouns).most_common(top_k)
    # return [w for w, _ in cnt]

    # [ë³€ê²½] ì „ì—­ 'okt' ì‚¬ìš©
    if not okt:
        return ["í•œêµ­ì–´ í‚¤ì›Œë“œ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨"]
    nouns = [n for n in okt.nouns(text) if len(n) > 1]  # 2ê¸€ì ì´ìƒ
    cnt = Counter(nouns).most_common(top_k)
    return [w for w, _ in cnt]


# ---------------------------  5) ì „ì²´ íŒŒì´í”„ë¼ì¸ --------------------------- #


def process_lyrics(lyrics: str) -> Tuple[str, List[str]]:
    lang = detect_language(lyrics)
    if lang == "en":
        en_summary = summarize_en(lyrics)
        summary_ko = translate_to_ko(en_summary)
        kws = keywords_en(lyrics)
    else:
        summary_ko = summarize_ko(lyrics)
        kws = keywords_ko(lyrics)
    return summary_ko, kws


# ---------------------------  6) ì‹¤í–‰ ì§„ì…ì  --------------------------- #
def main(doc_id: str, top_k: int = 10) -> None:
    """Firestoreì—ì„œ Document IDë¡œ ê°€ì‚¬ ë°ì´í„°ë¥¼ ê°€ì ¸ì™€ ë¶„ì„í•©ë‹ˆë‹¤."""
    # --- [ë³€ê²½] Firebase ì´ˆê¸°í™”ë¥¼ main í•¨ìˆ˜ ë‚´ë¶€ë¡œ ì´ë™ ---
    import firebase_admin
    from firebase_admin import credentials, firestore

    try:
        if not firebase_admin._apps:
            cred = credentials.ApplicationDefault()
            firebase_admin.initialize_app(cred)
            print("âœ… (Main) Firebase App initialized successfully.")
    except Exception as e:
        print(f"âŒ (Main) Firebase App initialization failed: {e}")
    db = firestore.client()
    # ---------------------------------------------------

    try:
        doc_ref = db.collection("user_playlists").document(doc_id)
        doc = doc_ref.get()

        if not doc.exists:
            raise FileNotFoundError(
                f"Firestoreì—ì„œ Document ID '{doc_id}'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            )

        data = doc.to_dict()
        songs = data.get(
            "tracks", []
        )  # get_lyrics_save_firestore.pyì—ì„œ 'tracks' í•„ë“œì— ì €ì¥

        if not songs:
            print("í•´ë‹¹ ë¬¸ì„œì— ë¶„ì„í•  ê³¡ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return

        for song in songs:
            title = song.get("clean_title") or song.get("original_title")
            artist = song.get("artist", "Unknown")
            lyrics = song.get("lyrics_processed") or song.get("lyrics") or ""

            if not lyrics:
                print(f"\n[{title} - {artist}] ê°€ì‚¬ ì •ë³´ê°€ ì—†ì–´ ë¶„ì„ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
                continue

            summary, kw = process_lyrics(lyrics)

            # ----- ê²°ê³¼ ì¶œë ¥ ----- #
            print(f"\n[{title} - {artist}]")
            print(f"ìš”ì•½ (3ë¬¸ì¥): {summary}")
            print(f"ì£¼ìš” ë‹¨ì–´ {top_k}ê°œ: {', '.join(kw)}")

    except Exception as e:
        print(f"ë°ì´í„° ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")


# ë¡œì»¬ ì‹¤í–‰ ë°©ë²•
# python lyrics_analyzer.py <your-firestore-document-id>
if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="Firestoreì—ì„œ ê°€ì‚¬ ë°ì´í„°ë¥¼ ê°€ì ¸ì™€ ë¶„ì„í•©ë‹ˆë‹¤."
    )
    parser.add_argument("doc_id", type=str, help="Firestoreì˜ Document ID")
    parser.add_argument("--top_k", type=int, default=10, help="ì¶”ì¶œí•  ì£¼ìš” ë‹¨ì–´ì˜ ìˆ˜")
    args = parser.parse_args()

    main(doc_id=args.doc_id, top_k=args.top_k)

# python lyrics_analyzer_firestore.py 85e90cd7-319e-4f00-87de-f4bebd4518ac
