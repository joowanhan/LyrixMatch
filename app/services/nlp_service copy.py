# lyrics_analyzer.py (Refactored for Eager Loading & Robustness)

#!/usr/bin/env python
# -*- coding: utf-8 -*-


import argparse
import json
import os
import re
from collections import Counter
from typing import List, Tuple
import nltk
import deepl
from sklearn.feature_extraction.text import CountVectorizer

# --- [Eager Loading] 1. ë¬´ê±°ìš´ ëª¨ë“ˆì„ ìµœìƒë‹¨ìœ¼ë¡œ ì´ë™ ---
import torch
from transformers import pipeline, AutoModelForSeq2SeqLM, AutoTokenizer
from konlpy.tag import Okt

# ---------------------------------------------------

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# --- ê²½ê³  ë©”ì‹œì§€ ìˆ¨ê¸°ê¸° ---
os.environ["TOKENIZERS_PARALLELISM"] = "false"

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# í™˜ê²½ ë³€ìˆ˜ / í† í° ì„¤ì •
from dotenv import load_dotenv

load_dotenv()

DEEPL_KEY = os.environ.get("DEEPL_KEY")
BART_PATH = "./models/bart"
T5_PATH = "./models/eenzeenee_t5"

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# --- [Eager Loading] 2. ëª¨ë¸/ê°ì²´ë¥¼ Noneìœ¼ë¡œ ì „ì—­ ì„ ì–¸ ---
print("â„¹ï¸ [Global Init] Declaring model variables as None.")
_summarizer_bart_pipeline = None
_tokenizer_t5 = None
_model_t5 = None
_translator_deepl = None
_vectorizer_en = None
_okt = None
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


# --- [Eager Loading] 3. ëª¨ë“  ëª¨ë¸ì„ ë¯¸ë¦¬ ë¡œë“œí•˜ëŠ” í•¨ìˆ˜ ì‹ ì„¤ ---
def load_all_models():
    """ì„œë²„ ì‹œì‘ ì‹œ í˜¸ì¶œë  í•¨ìˆ˜. ëª¨ë“  AI ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ë¥¼ ë©”ëª¨ë¦¬ì— ë¡œë“œ."""
    global _summarizer_bart_pipeline, _tokenizer_t5, _model_t5
    global _translator_deepl, _vectorizer_en, _okt

    print("ğŸ”„ [Eager Load] Starting to load all models...")

    try:
        # 1. BART (ì˜ì–´ ìš”ì•½)
        if _summarizer_bart_pipeline is None:
            print("ğŸ”„ [Eager Load] Loading BART Model...")
            tokenizer = AutoTokenizer.from_pretrained(BART_PATH)
            model = AutoModelForSeq2SeqLM.from_pretrained(BART_PATH).to("cpu")
            _summarizer_bart_pipeline = pipeline(
                "summarization", model=model, tokenizer=tokenizer
            )
            print("âœ… BART Model loaded.")

        # 2. T5 (í•œêµ­ì–´ ìš”ì•½)
        if _tokenizer_t5 is None or _model_t5 is None:
            print("ğŸ”„ [Eager Load] Loading T5 Model...")
            _tokenizer_t5 = AutoTokenizer.from_pretrained(T5_PATH)
            _model_t5 = AutoModelForSeq2SeqLM.from_pretrained(T5_PATH).to("cpu")
            print("âœ… T5 Model loaded.")

        # 3. DeepL (ë²ˆì—­)
        if _translator_deepl is None and DEEPL_KEY:
            print("ğŸ”„ [Eager Load] Loading DeepL Translator...")
            _translator_deepl = deepl.Translator(DEEPL_KEY)
            print("âœ… DeepL Translator loaded.")
        elif not DEEPL_KEY:
            print("â„¹ï¸ [Eager Load] DEEPL_KEY not set. Skipping DeepL.")

        # 4. CountVectorizer (ì˜ì–´ í‚¤ì›Œë“œ)
        if _vectorizer_en is None:
            print("ğŸ”„ [Eager Load] Loading English Keyword Vectorizer...")
            _vectorizer_en = CountVectorizer(
                stop_words="english",
                token_pattern=r"(?u)\b[a-zA-Z]{3,}\b",
            )
            print("âœ… English Keyword Vectorizer loaded.")

        # 5. Okt (í•œêµ­ì–´ í‚¤ì›Œë“œ)
        if _okt is None:
            print("ğŸ”„ [Eager Load] Loading Korean (Okt) Tokenizer...")
            _okt = Okt()
            print("âœ… Korean (Okt) Tokenizer loaded.")

        print("ğŸ‰ [Eager Load] All models loaded successfully.")

    except Exception as e:
        print(f"âŒ [Eager Load] Critical error during model loading: {e}")
        # ë¡œë“œ ì‹¤íŒ¨ ì‹œ, ì„œë²„ê°€ ì‹œì‘ë˜ì§€ ì•Šë„ë¡ ì˜ˆì™¸ë¥¼ ë‹¤ì‹œ ë°œìƒì‹œí‚¬ ìˆ˜ ìˆìŒ
        raise e


# ---------------------------  1) ì–¸ì–´ ê°ì§€ --------------------------- #


def detect_language(text: str, hangul_weight: float = 0.5) -> str:
    """ê°€ì‚¬ì—ì„œ í•œê¸€Â·ì˜ë¬¸ ë¬¸ì ë¹„ìœ¨ë¡œ â€˜koâ€™/â€˜enâ€™ ë°˜í™˜."""
    hangul = re.findall(r"[ê°€-í£]", text)
    latin = re.findall(r"[A-Za-z]", text)
    if len(hangul) + len(latin) == 0:
        return "en"
    return "ko" if len(hangul) / (len(hangul) + len(latin)) >= hangul_weight else "en"


# ---------------------------  2) ìš”ì•½ --------------------------- #
# [ìˆ˜ì •] Lazy Loading ë¡œì§ ì œê±°. _summarizer_bart_pipelineì´ ì´ë¯¸ ë¡œë“œë˜ì—ˆë‹¤ê³  ê°€ì •.
def summarize_en(text: str, max_len: int = 90, min_len: int = 25) -> str:
    global _summarizer_bart_pipeline
    if _summarizer_bart_pipeline is None:
        raise Exception("BART model is not loaded.")  # Eager Loading ì‹¤íŒ¨ ì‹œ
    with torch.no_grad():
        summary = _summarizer_bart_pipeline(
            text, min_length=min_len, max_length=max_len, do_sample=False
        )[0]["summary_text"]
    return summary.strip()


# [ìˆ˜ì •] Lazy Loading ë¡œì§ ì œê±°. _tokenizer_t5ì™€ _model_t5ê°€ ì´ë¯¸ ë¡œë“œë˜ì—ˆë‹¤ê³  ê°€ì •.
def summarize_ko(text: str, max_len: int = 64, min_len: int = 10) -> str:
    global _tokenizer_t5, _model_t5
    if _tokenizer_t5 is None or _model_t5 is None:
        raise Exception("T5 model is not loaded.")  # Eager Loading ì‹¤íŒ¨ ì‹œ

    prefix = "summarize: "
    input_text = prefix + text.replace("\n", " ").strip()
    inputs = _tokenizer_t5(
        [input_text], max_length=512, truncation=True, return_tensors="pt"
    )

    with torch.no_grad():
        output = _model_t5.generate(
            **inputs,
            num_beams=3,
            do_sample=True,
            min_length=min_len,
            max_length=max_len,
            early_stopping=True,
        )
    decoded = _tokenizer_t5.batch_decode(output, skip_special_tokens=True)[0].strip()
    sentences = nltk.sent_tokenize(decoded)
    return " ".join(sentences[:3])


# ---------------------------  3) DeepL ë²ˆì—­ --------------------------- #
# [ìˆ˜ì •] Lazy Loading ë¡œì§ ì œê±°.
def translate_to_ko(text: str) -> str:
    global _translator_deepl
    if not _translator_deepl:
        # print("ë²ˆì—­ê¸° ì—†ìŒ. ì˜ì–´ ìš”ì•½ ì›ë³¸ ë°˜í™˜.")
        return text
    return _translator_deepl.translate_text(text, target_lang="KO").text


# ---------------------------  4) ì£¼ìš” ë‹¨ì–´ ì¶”ì¶œ --------------------------- #
# [ìˆ˜ì •] Lazy Loading ë¡œì§ ì œê±°.
def keywords_en(
    text: str, title: str, top_k: int = 10
) -> List[str]:  # [ë³€ê²½] title ì¸ì ì¶”ê°€
    """ì˜ì–´ ê°€ì‚¬ì™€ ì œëª©ì„ ë°›ì•„, ì œëª©ì„ ì œì™¸í•œ ì£¼ìš” ë‹¨ì–´ Kê°œë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    global _vectorizer_en
    if _vectorizer_en is None:
        raise Exception("English Vectorizer is not loaded.")

    # [ì¶”ê°€] 1. ì œëª©ì—ì„œ í•„í„°ë§í•  ë‹¨ì–´(ì†Œë¬¸ì) set ìƒì„±
    # CountVectorizerì˜ í† í° íŒ¨í„´ê³¼ ìœ ì‚¬í•˜ê²Œ ì˜ì–´ ë‹¨ì–´ë§Œ ì¶”ì¶œ
    title_words = set(re.findall(r"(?u)\b[a-zA-Z]+\b", title.lower()))

    X = _vectorizer_en.fit_transform([text.lower()])
    counts = X.toarray().sum(axis=0)
    vocab = _vectorizer_en.get_feature_names_out()
    freq = sorted(zip(vocab, counts), key=lambda x: x[1], reverse=True)

    # [ì¶”ê°€] 2. freq ë¦¬ìŠ¤íŠ¸ì—ì„œ ì œëª© ë‹¨ì–´ í•„í„°ë§
    # _vectorizer_enì— ì˜í•´ vocab(w)ì€ ì´ë¯¸ ì†Œë¬¸ì, 3ê¸€ì ì´ìƒ, ë¶ˆìš©ì–´ ì œê±°ë¨
    filtered_freq = [(w, c) for w, c in freq if w not in title_words]

    # [ë³€ê²½] í•„í„°ë§ëœ ë¦¬ìŠ¤íŠ¸(filtered_freq)ì—ì„œ top_k ë°˜í™˜
    return [w for w, _ in filtered_freq[:top_k]]


# [ìˆ˜ì •] Lazy Loading ë¡œì§ ì œê±°.
def keywords_ko(
    text: str, title: str, top_k: int = 10
) -> List[str]:  # [ë³€ê²½] title ì¸ì ì¶”ê°€
    """í•œêµ­ì–´ ê°€ì‚¬ì™€ ì œëª©ì„ ë°›ì•„, ì œëª©ì„ ì œì™¸í•œ ì£¼ìš” ë‹¨ì–´ Kê°œë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    global _okt
    if _okt is None:
        raise Exception("Korean (Okt) Tokenizer is not loaded.")

    # [ì¶”ê°€] 1. ì œëª©ì—ì„œ í•„í„°ë§í•  ëª…ì‚¬ set ìƒì„± (2ê¸€ì ì´ìƒ)
    # ì œëª©ë„ ê°€ì‚¬ì™€ ë™ì¼í•œ ê¸°ì¤€ìœ¼ë¡œ ëª…ì‚¬ ì¶”ì¶œ
    title_nouns = set([n for n in _okt.nouns(title) if len(n) > 1])

    # [ë³€ê²½] 2. ê°€ì‚¬ ëª…ì‚¬ ì¶”ì¶œ ì‹œ ì œëª© ëª…ì‚¬(title_nouns)ì— ì—†ëŠ” ê²ƒë§Œ í•„í„°ë§
    nouns = [n for n in _okt.nouns(text) if len(n) > 1 and n not in title_nouns]

    cnt = Counter(nouns).most_common(top_k)
    return [w for w, _ in cnt]


# ---------------------------  5) ì „ì²´ íŒŒì´í”„ë¼ì¸ --------------------------- #


# [ìˆ˜ì •] ê°œë³„ ê³¡ ë¶„ì„ ì‹¤íŒ¨ ì‹œ 500 ì˜¤ë¥˜ ëŒ€ì‹  ê¸°ë³¸ê°’ì„ ë°˜í™˜í•˜ë„ë¡ try-except ì¶”ê°€
def process_lyrics(
    lyrics: str, title: str
) -> Tuple[str, List[str]]:  # [ë³€ê²½] title ì¸ì ì¶”ê°€
    """
    ê°€ì‚¬ì™€ ì œëª©ì„ ë°›ì•„ ìš”ì•½ê³¼ (ì œëª©ì´ í•„í„°ë§ëœ) í‚¤ì›Œë“œë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    [Robustness] ëª¨ë¸ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ ì‹œ, ë¹ˆ ë¬¸ìì—´ê³¼ ë¹ˆ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    try:
        lang = detect_language(lyrics)
        if lang == "en":
            # --- ì˜ì–´ ê°€ì‚¬ ì²˜ë¦¬ ---
            en_summary = summarize_en(lyrics)
            summary_ko = translate_to_ko(en_summary)
            # ì˜ì–´ ê°€ì‚¬ -> ì˜ì–´ ì œëª©(ì›ë³¸)ìœ¼ë¡œ í•„í„°ë§
            kws = keywords_en(lyrics, title=title, top_k=10)
        else:
            # --- í•œêµ­ì–´ ê°€ì‚¬ ì²˜ë¦¬ ---
            summary_ko = summarize_ko(lyrics)
            # 'title' (ì˜ˆ: "All For You")ì„ í•œêµ­ì–´ë¡œ ë²ˆì—­ (ì˜ˆ: "ë„ˆë¥¼ ìœ„í•˜ì—¬")
            # _translator_deeplì´ Noneì´ë©´ ì›ë³¸(ì˜ì–´) titleì´ ê·¸ëŒ€ë¡œ ì „ë‹¬ë¨ (Robustness)
            translated_title_ko = translate_to_ko(title)

            # í•œêµ­ì–´ ê°€ì‚¬ -> 'ë²ˆì—­ëœ í•œêµ­ì–´ ì œëª©'ìœ¼ë¡œ í•„í„°ë§
            kws = keywords_ko(lyrics, title=translated_title_ko, top_k=10)

        return summary_ko, kws

    except Exception as e:
        # ê°œë³„ ê³¡ ë¶„ì„ ì‹¤íŒ¨ ì‹œ (ì˜ˆ: "index out of range in self")
        print(f"âš ï¸  [Analysis Error] Failed to process single lyric: {e}")
        # ì„œë²„ ì¤‘ë‹¨ ëŒ€ì‹ , ì´ ê³¡ì— ëŒ€í•œ ë¹ˆ ê²°ê³¼ë¥¼ ë°˜í™˜
        return "", []


# ---------------------------  6) ì‹¤í–‰ ì§„ì…ì  --------------------------- #
def main(doc_id: str, top_k: int = 10) -> None:
    """Firestoreì—ì„œ Document IDë¡œ ê°€ì‚¬ ë°ì´í„°ë¥¼ ê°€ì ¸ì™€ ë¶„ì„í•©ë‹ˆë‹¤."""
    # --- [ë³€ê²½] Firebase ì´ˆê¸°í™”ë¥¼ main í•¨ìˆ˜ ë‚´ë¶€ë¡œ ì´ë™ ---
    import firebase_admin
    from firebase_admin import credentials, firestore

    try:
        if not firebase_admin._apps:
            firebase_admin.initialize_app()
            print("âœ… Firebase App initialized successfully (from module).")
    except Exception as e:
        print(f"âŒ Firebase App initialization failed in module: {e}")

    db = firestore.client()
    # ---------------------------------------------------

    # --- [Eager Loading] 5. ë¡œì»¬ ì‹¤í–‰ ì‹œì—ë„ ëª¨ë¸ ë¡œë“œ ---
    # (api_server.pyê°€ ì•„ë‹Œ, ì´ íŒŒì¼ì„ ì§ì ‘ ì‹¤í–‰í•  ê²½ìš°)
    load_all_models()
    # -------------------------------------------------

    try:
        doc_ref = db.collection("user_playlists").document(doc_id)
        doc = doc_ref.get()

        if not doc.exists:
            raise FileNotFoundError(
                f"Firestoreì—ì„œ Document ID '{doc_id}'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            )

        data = doc.to_dict()
        songs = data.get("tracks", [])

        if not songs:
            print("í•´ë‹¹ ë¬¸ì„œì— ë¶„ì„í•  ê³¡ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return

        for song in songs:
            title = song.get("clean_title") or song.get("original_title") or ""
            artist = song.get("artist", "Unknown")
            lyrics = song.get("lyrics_processed") or song.get("lyrics") or ""

            if not lyrics:
                print(f"\n[{title} - {artist}] ê°€ì‚¬ ì •ë³´ê°€ ì—†ì–´ ë¶„ì„ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
                continue

            # [ë³€ê²½] process_lyrics í˜¸ì¶œ ì‹œ title ì „ë‹¬
            summary, kw = process_lyrics(lyrics, title=title)

            # ----- ê²°ê³¼ ì¶œë ¥ ----- #
            print(f"\n[{title} - {artist}]")
            print(f"ìš”ì•½ (3ë¬¸ì¥): {summary}")
            print(f"ì£¼ìš” ë‹¨ì–´ {top_k}ê°œ: {', '.join(kw)}")

    except Exception as e:
        print(f"ë°ì´í„° ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")


# ë¡œì»¬ ì‹¤í–‰ ë°©ë²•
# python lyrics_analyzer.py <your-firestore-document-id>
if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="Firestoreì—ì„œ ê°€ì‚¬ ë°ì´í„°ë¥¼ ê°€ì ¸ì™€ ë¶„ì„í•©ë‹ˆë‹¤."
    )
    parser.add_argument("doc_id", type=str, help="Firestoreì˜ Document ID")
    parser.add_argument("--top_k", type=int, default=10, help="ì¶”ì¶œí•  ì£¼ìš” ë‹¨ì–´ì˜ ìˆ˜")
    args = parser.parse_args()

    main(doc_id=args.doc_id, top_k=args.top_k)
